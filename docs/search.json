[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jayjit Das",
    "section": "",
    "text": "Student and practitioner of data science and ecology with &gt; 5 years of eclectic experience across sectors and geographies. Passionate to work on projects involving data-driven decision making and storytelling via data visualization."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Childcare Cost Prediction using XGBoost and early stopping approach\n\n\n\n\n\n\nJayjit Das\n\n\n\n\n\n\n\n\n\n\n\n\nChildcare Cost Prediction using XGBoost and early stopping approach\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChildcare Cost Prediction using XGBoost and early stopping approach\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHimalayan Expedition Survival Prediction\n\n\nAn analysis of survival prediction in Himalayan expeditions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Volcano Classification project\n\n\n\n\n\n\nJayjit Das\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Child health cost/Childcare cost.html",
    "href": "posts/Child health cost/Childcare cost.html",
    "title": "Childcare Cost Prediction using XGBoost and early stopping approach",
    "section": "",
    "text": "The emphasis of this project is on the utilization of tidymodels framework for the optimization of an xgboost model through early stopping, employing the current week’s #TidyTuesday dataset on childcare expenses in the United States.\n\n\n\n\nCode\nlibrary(tidyverse)\n\nchildcare_costs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-09/childcare_costs.csv')\n\nglimpse(childcare_costs)\n\n\nRows: 34,567\nColumns: 61\n$ county_fips_code          &lt;dbl&gt; 1001, 1001, 1001, 1001, 1001, 1001, 1001, 10…\n$ study_year                &lt;dbl&gt; 2008, 2009, 2010, 2011, 2012, 2013, 2014, 20…\n$ unr_16                    &lt;dbl&gt; 5.42, 5.93, 6.21, 7.55, 8.60, 9.39, 8.50, 7.…\n$ funr_16                   &lt;dbl&gt; 4.41, 5.72, 5.57, 8.13, 8.88, 10.31, 9.18, 8…\n$ munr_16                   &lt;dbl&gt; 6.32, 6.11, 6.78, 7.03, 8.29, 8.56, 7.95, 6.…\n$ unr_20to64                &lt;dbl&gt; 4.6, 4.8, 5.1, 6.2, 6.7, 7.3, 6.8, 5.9, 4.4,…\n$ funr_20to64               &lt;dbl&gt; 3.5, 4.6, 4.6, 6.3, 6.4, 7.6, 6.8, 6.1, 4.6,…\n$ munr_20to64               &lt;dbl&gt; 5.6, 5.0, 5.6, 6.1, 7.0, 7.0, 6.8, 5.9, 4.3,…\n$ flfpr_20to64              &lt;dbl&gt; 68.9, 70.8, 71.3, 70.2, 70.6, 70.7, 69.9, 68…\n$ flfpr_20to64_under6       &lt;dbl&gt; 66.9, 63.7, 67.0, 66.5, 67.1, 67.5, 65.2, 66…\n$ flfpr_20to64_6to17        &lt;dbl&gt; 79.59, 78.41, 78.15, 77.62, 76.31, 75.91, 75…\n$ flfpr_20to64_under6_6to17 &lt;dbl&gt; 60.81, 59.91, 59.71, 59.31, 58.30, 58.00, 57…\n$ mlfpr_20to64              &lt;dbl&gt; 84.0, 86.2, 85.8, 85.7, 85.7, 85.0, 84.2, 82…\n$ pr_f                      &lt;dbl&gt; 8.5, 7.5, 7.5, 7.4, 7.4, 8.3, 9.1, 9.3, 9.4,…\n$ pr_p                      &lt;dbl&gt; 11.5, 10.3, 10.6, 10.9, 11.6, 12.1, 12.8, 12…\n$ mhi_2018                  &lt;dbl&gt; 58462.55, 60211.71, 61775.80, 60366.88, 5915…\n$ me_2018                   &lt;dbl&gt; 32710.60, 34688.16, 34740.84, 34564.32, 3432…\n$ fme_2018                  &lt;dbl&gt; 25156.25, 26852.67, 27391.08, 26727.68, 2796…\n$ mme_2018                  &lt;dbl&gt; 41436.80, 43865.64, 46155.24, 45333.12, 4427…\n$ total_pop                 &lt;dbl&gt; 49744, 49584, 53155, 53944, 54590, 54907, 55…\n$ one_race                  &lt;dbl&gt; 98.1, 98.6, 98.5, 98.5, 98.5, 98.6, 98.7, 98…\n$ one_race_w                &lt;dbl&gt; 78.9, 79.1, 79.1, 78.9, 78.9, 78.3, 78.0, 77…\n$ one_race_b                &lt;dbl&gt; 17.7, 17.9, 17.9, 18.1, 18.1, 18.4, 18.6, 18…\n$ one_race_i                &lt;dbl&gt; 0.4, 0.4, 0.3, 0.2, 0.3, 0.3, 0.4, 0.4, 0.4,…\n$ one_race_a                &lt;dbl&gt; 0.4, 0.6, 0.7, 0.7, 0.8, 1.0, 0.9, 1.0, 0.8,…\n$ one_race_h                &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1,…\n$ one_race_other            &lt;dbl&gt; 0.7, 0.7, 0.6, 0.5, 0.4, 0.7, 0.7, 0.9, 1.4,…\n$ two_races                 &lt;dbl&gt; 1.9, 1.4, 1.5, 1.5, 1.5, 1.4, 1.3, 1.6, 2.0,…\n$ hispanic                  &lt;dbl&gt; 1.8, 2.0, 2.3, 2.4, 2.4, 2.5, 2.5, 2.6, 2.6,…\n$ households                &lt;dbl&gt; 18373, 18288, 19718, 19998, 19934, 20071, 20…\n$ h_under6_both_work        &lt;dbl&gt; 1543, 1475, 1569, 1695, 1714, 1532, 1557, 13…\n$ h_under6_f_work           &lt;dbl&gt; 970, 964, 1009, 1060, 938, 880, 1191, 1258, …\n$ h_under6_m_work           &lt;dbl&gt; 22, 16, 16, 106, 120, 161, 159, 211, 109, 10…\n$ h_under6_single_m         &lt;dbl&gt; 995, 1099, 1110, 1030, 1095, 1160, 954, 883,…\n$ h_6to17_both_work         &lt;dbl&gt; 4900, 5028, 5472, 5065, 4608, 4238, 4056, 40…\n$ h_6to17_fwork             &lt;dbl&gt; 1308, 1519, 1541, 1965, 1963, 1978, 2073, 20…\n$ h_6to17_mwork             &lt;dbl&gt; 114, 92, 113, 246, 284, 354, 373, 551, 322, …\n$ h_6to17_single_m          &lt;dbl&gt; 1966, 2305, 2377, 2299, 2644, 2522, 2269, 21…\n$ emp_m                     &lt;dbl&gt; 27.40, 29.54, 29.33, 31.17, 32.13, 31.74, 32…\n$ memp_m                    &lt;dbl&gt; 24.41, 26.07, 25.94, 26.97, 28.59, 27.44, 28…\n$ femp_m                    &lt;dbl&gt; 30.68, 33.40, 33.06, 35.96, 36.09, 36.61, 37…\n$ emp_service               &lt;dbl&gt; 17.06, 15.81, 16.92, 16.18, 16.09, 16.72, 16…\n$ memp_service              &lt;dbl&gt; 15.53, 14.16, 15.09, 14.21, 14.71, 13.92, 13…\n$ femp_service              &lt;dbl&gt; 18.75, 17.64, 18.93, 18.42, 17.63, 19.89, 20…\n$ emp_sales                 &lt;dbl&gt; 29.11, 28.75, 29.07, 27.56, 28.39, 27.22, 25…\n$ memp_sales                &lt;dbl&gt; 15.97, 17.51, 17.82, 17.74, 17.79, 17.38, 15…\n$ femp_sales                &lt;dbl&gt; 43.52, 41.25, 41.43, 38.76, 40.26, 38.36, 36…\n$ emp_n                     &lt;dbl&gt; 13.21, 11.89, 11.57, 10.72, 9.02, 9.27, 9.38…\n$ memp_n                    &lt;dbl&gt; 22.54, 20.30, 19.86, 18.28, 16.03, 16.79, 17…\n$ femp_n                    &lt;dbl&gt; 2.99, 2.52, 2.45, 2.09, 1.19, 0.77, 0.58, 0.…\n$ emp_p                     &lt;dbl&gt; 13.22, 14.02, 13.11, 14.38, 14.37, 15.04, 16…\n$ memp_p                    &lt;dbl&gt; 21.55, 21.96, 21.28, 22.80, 22.88, 24.48, 24…\n$ femp_p                    &lt;dbl&gt; 4.07, 5.19, 4.13, 4.77, 4.84, 4.36, 6.07, 7.…\n$ mcsa                      &lt;dbl&gt; 80.92, 83.42, 85.92, 88.43, 90.93, 93.43, 95…\n$ mfccsa                    &lt;dbl&gt; 81.40, 85.68, 89.96, 94.25, 98.53, 102.82, 1…\n$ mc_infant                 &lt;dbl&gt; 104.95, 105.11, 105.28, 105.45, 105.61, 105.…\n$ mc_toddler                &lt;dbl&gt; 104.95, 105.11, 105.28, 105.45, 105.61, 105.…\n$ mc_preschool              &lt;dbl&gt; 85.92, 87.59, 89.26, 90.93, 92.60, 94.27, 95…\n$ mfcc_infant               &lt;dbl&gt; 83.45, 87.39, 91.33, 95.28, 99.22, 103.16, 1…\n$ mfcc_toddler              &lt;dbl&gt; 83.45, 87.39, 91.33, 95.28, 99.22, 103.16, 1…\n$ mfcc_preschool            &lt;dbl&gt; 81.40, 85.68, 89.96, 94.25, 98.53, 102.82, 1…"
  },
  {
    "objectID": "posts/Child health cost/Childcare cost.html#exploratory-data-analysis",
    "href": "posts/Child health cost/Childcare cost.html#exploratory-data-analysis",
    "title": "Childcare Cost Prediction using XGBoost and early stopping approach",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nchildcare_costs &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-05-09/childcare_costs.csv')\n\nglimpse(childcare_costs)\n\n\nRows: 34,567\nColumns: 61\n$ county_fips_code          &lt;dbl&gt; 1001, 1001, 1001, 1001, 1001, 1001, 1001, 10…\n$ study_year                &lt;dbl&gt; 2008, 2009, 2010, 2011, 2012, 2013, 2014, 20…\n$ unr_16                    &lt;dbl&gt; 5.42, 5.93, 6.21, 7.55, 8.60, 9.39, 8.50, 7.…\n$ funr_16                   &lt;dbl&gt; 4.41, 5.72, 5.57, 8.13, 8.88, 10.31, 9.18, 8…\n$ munr_16                   &lt;dbl&gt; 6.32, 6.11, 6.78, 7.03, 8.29, 8.56, 7.95, 6.…\n$ unr_20to64                &lt;dbl&gt; 4.6, 4.8, 5.1, 6.2, 6.7, 7.3, 6.8, 5.9, 4.4,…\n$ funr_20to64               &lt;dbl&gt; 3.5, 4.6, 4.6, 6.3, 6.4, 7.6, 6.8, 6.1, 4.6,…\n$ munr_20to64               &lt;dbl&gt; 5.6, 5.0, 5.6, 6.1, 7.0, 7.0, 6.8, 5.9, 4.3,…\n$ flfpr_20to64              &lt;dbl&gt; 68.9, 70.8, 71.3, 70.2, 70.6, 70.7, 69.9, 68…\n$ flfpr_20to64_under6       &lt;dbl&gt; 66.9, 63.7, 67.0, 66.5, 67.1, 67.5, 65.2, 66…\n$ flfpr_20to64_6to17        &lt;dbl&gt; 79.59, 78.41, 78.15, 77.62, 76.31, 75.91, 75…\n$ flfpr_20to64_under6_6to17 &lt;dbl&gt; 60.81, 59.91, 59.71, 59.31, 58.30, 58.00, 57…\n$ mlfpr_20to64              &lt;dbl&gt; 84.0, 86.2, 85.8, 85.7, 85.7, 85.0, 84.2, 82…\n$ pr_f                      &lt;dbl&gt; 8.5, 7.5, 7.5, 7.4, 7.4, 8.3, 9.1, 9.3, 9.4,…\n$ pr_p                      &lt;dbl&gt; 11.5, 10.3, 10.6, 10.9, 11.6, 12.1, 12.8, 12…\n$ mhi_2018                  &lt;dbl&gt; 58462.55, 60211.71, 61775.80, 60366.88, 5915…\n$ me_2018                   &lt;dbl&gt; 32710.60, 34688.16, 34740.84, 34564.32, 3432…\n$ fme_2018                  &lt;dbl&gt; 25156.25, 26852.67, 27391.08, 26727.68, 2796…\n$ mme_2018                  &lt;dbl&gt; 41436.80, 43865.64, 46155.24, 45333.12, 4427…\n$ total_pop                 &lt;dbl&gt; 49744, 49584, 53155, 53944, 54590, 54907, 55…\n$ one_race                  &lt;dbl&gt; 98.1, 98.6, 98.5, 98.5, 98.5, 98.6, 98.7, 98…\n$ one_race_w                &lt;dbl&gt; 78.9, 79.1, 79.1, 78.9, 78.9, 78.3, 78.0, 77…\n$ one_race_b                &lt;dbl&gt; 17.7, 17.9, 17.9, 18.1, 18.1, 18.4, 18.6, 18…\n$ one_race_i                &lt;dbl&gt; 0.4, 0.4, 0.3, 0.2, 0.3, 0.3, 0.4, 0.4, 0.4,…\n$ one_race_a                &lt;dbl&gt; 0.4, 0.6, 0.7, 0.7, 0.8, 1.0, 0.9, 1.0, 0.8,…\n$ one_race_h                &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1,…\n$ one_race_other            &lt;dbl&gt; 0.7, 0.7, 0.6, 0.5, 0.4, 0.7, 0.7, 0.9, 1.4,…\n$ two_races                 &lt;dbl&gt; 1.9, 1.4, 1.5, 1.5, 1.5, 1.4, 1.3, 1.6, 2.0,…\n$ hispanic                  &lt;dbl&gt; 1.8, 2.0, 2.3, 2.4, 2.4, 2.5, 2.5, 2.6, 2.6,…\n$ households                &lt;dbl&gt; 18373, 18288, 19718, 19998, 19934, 20071, 20…\n$ h_under6_both_work        &lt;dbl&gt; 1543, 1475, 1569, 1695, 1714, 1532, 1557, 13…\n$ h_under6_f_work           &lt;dbl&gt; 970, 964, 1009, 1060, 938, 880, 1191, 1258, …\n$ h_under6_m_work           &lt;dbl&gt; 22, 16, 16, 106, 120, 161, 159, 211, 109, 10…\n$ h_under6_single_m         &lt;dbl&gt; 995, 1099, 1110, 1030, 1095, 1160, 954, 883,…\n$ h_6to17_both_work         &lt;dbl&gt; 4900, 5028, 5472, 5065, 4608, 4238, 4056, 40…\n$ h_6to17_fwork             &lt;dbl&gt; 1308, 1519, 1541, 1965, 1963, 1978, 2073, 20…\n$ h_6to17_mwork             &lt;dbl&gt; 114, 92, 113, 246, 284, 354, 373, 551, 322, …\n$ h_6to17_single_m          &lt;dbl&gt; 1966, 2305, 2377, 2299, 2644, 2522, 2269, 21…\n$ emp_m                     &lt;dbl&gt; 27.40, 29.54, 29.33, 31.17, 32.13, 31.74, 32…\n$ memp_m                    &lt;dbl&gt; 24.41, 26.07, 25.94, 26.97, 28.59, 27.44, 28…\n$ femp_m                    &lt;dbl&gt; 30.68, 33.40, 33.06, 35.96, 36.09, 36.61, 37…\n$ emp_service               &lt;dbl&gt; 17.06, 15.81, 16.92, 16.18, 16.09, 16.72, 16…\n$ memp_service              &lt;dbl&gt; 15.53, 14.16, 15.09, 14.21, 14.71, 13.92, 13…\n$ femp_service              &lt;dbl&gt; 18.75, 17.64, 18.93, 18.42, 17.63, 19.89, 20…\n$ emp_sales                 &lt;dbl&gt; 29.11, 28.75, 29.07, 27.56, 28.39, 27.22, 25…\n$ memp_sales                &lt;dbl&gt; 15.97, 17.51, 17.82, 17.74, 17.79, 17.38, 15…\n$ femp_sales                &lt;dbl&gt; 43.52, 41.25, 41.43, 38.76, 40.26, 38.36, 36…\n$ emp_n                     &lt;dbl&gt; 13.21, 11.89, 11.57, 10.72, 9.02, 9.27, 9.38…\n$ memp_n                    &lt;dbl&gt; 22.54, 20.30, 19.86, 18.28, 16.03, 16.79, 17…\n$ femp_n                    &lt;dbl&gt; 2.99, 2.52, 2.45, 2.09, 1.19, 0.77, 0.58, 0.…\n$ emp_p                     &lt;dbl&gt; 13.22, 14.02, 13.11, 14.38, 14.37, 15.04, 16…\n$ memp_p                    &lt;dbl&gt; 21.55, 21.96, 21.28, 22.80, 22.88, 24.48, 24…\n$ femp_p                    &lt;dbl&gt; 4.07, 5.19, 4.13, 4.77, 4.84, 4.36, 6.07, 7.…\n$ mcsa                      &lt;dbl&gt; 80.92, 83.42, 85.92, 88.43, 90.93, 93.43, 95…\n$ mfccsa                    &lt;dbl&gt; 81.40, 85.68, 89.96, 94.25, 98.53, 102.82, 1…\n$ mc_infant                 &lt;dbl&gt; 104.95, 105.11, 105.28, 105.45, 105.61, 105.…\n$ mc_toddler                &lt;dbl&gt; 104.95, 105.11, 105.28, 105.45, 105.61, 105.…\n$ mc_preschool              &lt;dbl&gt; 85.92, 87.59, 89.26, 90.93, 92.60, 94.27, 95…\n$ mfcc_infant               &lt;dbl&gt; 83.45, 87.39, 91.33, 95.28, 99.22, 103.16, 1…\n$ mfcc_toddler              &lt;dbl&gt; 83.45, 87.39, 91.33, 95.28, 99.22, 103.16, 1…\n$ mfcc_preschool            &lt;dbl&gt; 81.40, 85.68, 89.96, 94.25, 98.53, 102.82, 1…"
  },
  {
    "objectID": "posts/Child health cost/Childcare cost.html#building-a-model",
    "href": "posts/Child health cost/Childcare cost.html#building-a-model",
    "title": "Childcare Cost Prediction using XGBoost and early stopping approach",
    "section": "Building a model",
    "text": "Building a model\nThe initiation of our modeling process involves the establishment of our “data budget.” In this instance, the prediction target is mcsa (the costs associated with school-age children in childcare centers), necessitating the removal of other childcare cost measures related to babies, toddlers, and family-based childcare, among others. The FIPS codes, which explicitly encode location, are also to be removed, shifting the focus to county characteristics such as household income and the number of households with children. Given the substantial size of this dataset, it is suggested that a single validation set be utilized.\n\n\nCode\nlibrary(tidymodels)\n\nset.seed(111)\nchildcare_split &lt;- childcare_costs |&gt;\n  select(-matches(\"^mc_|^mfc\")) |&gt;\n  select(-county_fips_code) |&gt;\n  na.omit() |&gt;\n  initial_split(strata = mcsa)\n\nchildcare_train &lt;- training(childcare_split)\nchildcare_test &lt;- testing(childcare_split)\n\nset.seed(222)\nchildcare_set &lt;- validation_split(childcare_train)\nchildcare_set\n\n\n# Validation Set Split (0.75/0.25)  \n# A tibble: 1 × 2\n  splits               id        \n  &lt;list&gt;               &lt;chr&gt;     \n1 &lt;split [13269/4424]&gt; validation\n\n\nGiven that all predictors are already in numeric form, there is no requirement for any special feature engineering; a formula such as mcsa ~ … can be utilized. However, there is a need to establish a tunable xgboost model specification with early stopping. The number of trees will be maintained as a constant (and not excessively high), the stop_iter (the early stopping parameter) will be set to tune(), and a few other parameters will be tuned. It should be noted that a validation set needs to be established (which in this case is a proportion of the training set) to be reserved for determining when to stop.\n\n\nCode\nxgb_spec &lt;-\n  boost_tree(\n    trees = 500,\n    min_n = tune(),\n    mtry = tune(),\n    stop_iter = tune(),\n    learn_rate = 0.01\n  ) |&gt;\n  set_engine(\"xgboost\", validation = 0.2) |&gt;\n  set_mode(\"regression\")\n\nxgb_wf &lt;- workflow(mcsa ~ ., xgb_spec)\nxgb_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: boost_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nmcsa ~ .\n\n── Model ───────────────────────────────────────────────────────────────────────\nBoosted Tree Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n  trees = 500\n  min_n = tune()\n  learn_rate = 0.01\n  stop_iter = tune()\n\nEngine-Specific Arguments:\n  validation = 0.2\n\nComputational engine: xgboost \n\n\nThe model is prepared for deployment. The next step involves adjusting various hyperparameters using the training set, which includes a subset reserved for early stopping, in addition to our validation set.\n\n\nCode\ndoParallel::registerDoParallel()\nset.seed(234)\nxgb_rs &lt;- tune_grid(xgb_wf, childcare_set, grid = 15)\nxgb_rs\n\n\n# Tuning results\n# Validation Set Split (0.75/0.25)  \n# A tibble: 1 × 4\n  splits               id         .metrics          .notes          \n  &lt;list&gt;               &lt;chr&gt;      &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [13269/4424]&gt; validation &lt;tibble [30 × 7]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "posts/Child health cost/Childcare cost.html#evaluating-results",
    "href": "posts/Child health cost/Childcare cost.html#evaluating-results",
    "title": "Childcare Cost Prediction using XGBoost and early stopping approach",
    "section": "Evaluating results",
    "text": "Evaluating results\nLet’s see how this turned out.\n\n\nCode\nautoplot(xgb_rs)\n\n\n\n\n\nVisualizing the top results\n\n\nCode\nshow_best(xgb_rs, \"rmse\")\n\n\n# A tibble: 5 × 9\n   mtry min_n stop_iter .metric .estimator  mean     n std_err .config          \n  &lt;int&gt; &lt;int&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;            \n1    42     5        14 rmse    standard    21.7     1      NA Preprocessor1_Mo…\n2    35     2         9 rmse    standard    21.8     1      NA Preprocessor1_Mo…\n3    38     7        20 rmse    standard    21.8     1      NA Preprocessor1_Mo…\n4    41    17        16 rmse    standard    21.9     1      NA Preprocessor1_Mo…\n5    25    17         8 rmse    standard    21.9     1      NA Preprocessor1_Mo…\n\n\nThe optimal Root Mean Square Error (RMSE) is slightly over $20. This value provides an estimate of the accuracy with which we can predict the median childcare cost in a US county, given that the median cost in this dataset was approximately $100.\nWe will now employ the last_fit() function to fit the model one last time using the training data and evaluate its performance on the test data, utilizing the numerically optimal result obtained from xgb_rs.\n\n\nCode\nchildcare_fit &lt;- xgb_wf |&gt;\n  finalize_workflow(select_best(xgb_rs, \"rmse\")) |&gt;\n  last_fit(childcare_split)\n\nchildcare_fit\n\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits               id              .metrics .notes   .predictions .workflow \n  &lt;list&gt;               &lt;chr&gt;           &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [17693/5900]&gt; train/test spl… &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\nLet’s see how did this model perform on the testing data, that was not used in tuning or training.\n\n\nCode\ncollect_metrics(childcare_fit)\n\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard      21.4   Preprocessor1_Model1\n2 rsq     standard       0.621 Preprocessor1_Model1\n\n\nMost important features for the target variable in this xgboost model.\n\n\nCode\nlibrary(vip)\n\nextract_workflow(childcare_fit) |&gt;\n  extract_fit_parsnip() |&gt;\n  vip(num_features = 15, geom = \"point\")\n\n\n\n\n\nIn this model, significant influences are exerted by the percentage of the county’s population identified as Asian, the median income of households, the median earnings for women, the year, and the total number of households in the county."
  },
  {
    "objectID": "posts/Child health cost/Childcare cost.html#deploying-the-model",
    "href": "posts/Child health cost/Childcare cost.html#deploying-the-model",
    "title": "Childcare Cost Prediction using XGBoost and early stopping approach",
    "section": "Deploying the model",
    "text": "Deploying the model\nObject created for deploying the model.\n\n\nCode\nlibrary(vetiver)\nv &lt;- extract_workflow(childcare_fit) |&gt;\n  vetiver_model(\"childcare-costs-xgb\")\nv\n\n\n\n── childcare-costs-xgb ─ &lt;bundled_workflow&gt; model for deployment \nA xgboost regression modeling workflow using 52 features"
  },
  {
    "objectID": "posts/Volcano/Volcano project.html",
    "href": "posts/Volcano/Volcano project.html",
    "title": "Multinomial Volcano Classification project",
    "section": "",
    "text": "Our objective in modeling is to forecast the category of volcanoes in this week’s #TidyTuesday dataset, relying on various volcano characteristics such as latitude, longitude, tectonic setting, etc. Since there are more than two volcano types, this task falls under the category of multiclass or multinomial classification rather than binary classification.\n\n\nCode\nlibrary(tidyverse)\n\n\n\n\nCode\nvolcano_raw &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-12/volcano.csv\")\n\nvolcano_raw %&gt;%\n  count(primary_volcano_type, sort = TRUE)\n\n\n# A tibble: 26 × 2\n   primary_volcano_type     n\n   &lt;chr&gt;                &lt;int&gt;\n 1 Stratovolcano          353\n 2 Stratovolcano(es)      107\n 3 Shield                  85\n 4 Volcanic field          71\n 5 Pyroclastic cone(s)     70\n 6 Caldera                 65\n 7 Complex                 46\n 8 Shield(s)               33\n 9 Submarine               27\n10 Lava dome(s)            26\n# ℹ 16 more rows\n\n\nGiven the abundance of volcano types and our limited dataset of only 958 examples, building a model for each type might be challenging. Instead, let’s simplify the task by creating a new variable named volcano_type and focus on distinguishing between three main types:\n\nStratovolcano\nShield volcano\nEverything else (other)\n\nAs we employ the transmute() function to generate this new variable, let’s also choose the specific variables relevant for modeling. These variables will include information about the tectonics surrounding the volcano and the most crucial rock type.\n\n\nCode\nvolcano_df &lt;- volcano_raw %&gt;%\n  transmute(\n    volcano_type = case_when(\n      str_detect(primary_volcano_type, \"Stratovolcano\") ~ \"Stratovolcano\",\n      str_detect(primary_volcano_type, \"Shield\") ~ \"Shield\",\n      TRUE ~ \"Other\"\n    ),\n    volcano_number, latitude, longitude, elevation,\n    tectonic_settings, major_rock_1\n  ) %&gt;%\n  mutate_if(is.character, factor)\n\nvolcano_df %&gt;%\n  count(volcano_type, sort = TRUE)\n\n\n# A tibble: 3 × 2\n  volcano_type      n\n  &lt;fct&gt;         &lt;int&gt;\n1 Stratovolcano   461\n2 Other           379\n3 Shield          118\n\n\nWe will be building a multiclass predictive model since the papers are categorized into three groups: finance, microeconomics, and macroeconomics. Unlike the common use of binary classification models, our objective involves predicting among multiple classes. Before diving into the modeling process, let’s generate an exploratory plot.\nVisualizing the distribution of various volcano types.\n\n\nCode\nworld &lt;- map_data(\"world\")\n\nggplot() +\n  geom_map(\n    data = world, map = world,\n    aes(long, lat, map_id = region),\n    color = \"white\", fill = \"gray50\", size = 0.05, alpha = 0.2\n  ) +\n  geom_point(\n    data = volcano_df,\n    aes(longitude, latitude, color = volcano_type),\n    alpha = 0.8\n  ) +\n  theme_void(base_family = \"IBMPlexSans\") +\n  labs(x = NULL, y = NULL, color = NULL)\n\n\n\n\n\nThese type of relationships between category and title words are what we want to use in our predictive model.\n\n\nInstead of dividing this relatively small dataset into training and testing data, a set of bootstrap resamples will be generated.\n\n\nCode\nlibrary(tidymodels)\nvolcano_boot &lt;- bootstraps(volcano_df)\n\nvolcano_boot\n\n\n# Bootstrap sampling \n# A tibble: 25 × 2\n   splits            id         \n   &lt;list&gt;            &lt;chr&gt;      \n 1 &lt;split [958/352]&gt; Bootstrap01\n 2 &lt;split [958/358]&gt; Bootstrap02\n 3 &lt;split [958/362]&gt; Bootstrap03\n 4 &lt;split [958/360]&gt; Bootstrap04\n 5 &lt;split [958/347]&gt; Bootstrap05\n 6 &lt;split [958/358]&gt; Bootstrap06\n 7 &lt;split [958/347]&gt; Bootstrap07\n 8 &lt;split [958/362]&gt; Bootstrap08\n 9 &lt;split [958/344]&gt; Bootstrap09\n10 &lt;split [958/349]&gt; Bootstrap10\n# ℹ 15 more rows\n\n\nOur multinomial classification model will be trained on these resamples. Next, our data will be preprocessed using a recipe. Considering the substantial imbalance with significantly fewer shield volcanoes compared to the other groups, SMOTE upsampling (via the themis package) will be applied to balance the classes.\n\n\nCode\nlibrary(themis)\n\nvolcano_rec &lt;- recipe(volcano_type ~ ., data = volcano_df) %&gt;%\n  update_role(volcano_number, new_role = \"Id\") %&gt;%\n  step_other(tectonic_settings) %&gt;%\n  step_other(major_rock_1) %&gt;%\n  step_dummy(tectonic_settings, major_rock_1) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_smote(volcano_type)\n\n\n\nFirst, the recipe() function needs to be informed about the model using a formula and the data it will work with.\nFollowing that, an update is made to the role of the volcano number, designating it as a variable to retain for convenience as an identifier for rows, although it is neither a predictor nor an outcome.\nConsidering the numerous tectonic settings and rocks in the dataset, less frequently occurring levels are combined into an “Other” category for each predictor.\nSubsequently, indicator variables are created, and those with zero variance are removed.\nPrior to oversampling, all predictors are centered and scaled (normalized).\nLastly, SMOTE oversampling is implemented to ensure balance among the volcano types.\n\n\n\nCode\nvolcano_prep &lt;- prep(volcano_rec)\njuice(volcano_prep)\n\n\n# A tibble: 1,383 × 14\n   volcano_number latitude longitude elevation volcano_type \n            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;        \n 1         283001   0.618      0.984   -0.875  Shield       \n 2         355096  -1.21      -0.830    2.97   Stratovolcano\n 3         342080  -0.0153    -1.04     1.50   Stratovolcano\n 4         213004   0.746      0.101   -0.131  Other        \n 5         321040   0.988     -1.32     1.34   Stratovolcano\n 6         283170   0.718      1.06    -0.0992 Stratovolcano\n 7         221170  -0.156      0.158   -0.0956 Stratovolcano\n 8         221110  -0.0601     0.158   -0.440  Stratovolcano\n 9         284160   0.120      1.11    -0.644  Stratovolcano\n10         342100  -0.0165    -1.04     1.35   Stratovolcano\n# ℹ 1,373 more rows\n# ℹ 9 more variables:\n#   tectonic_settings_Rift.zone...Continental.crust...25.km. &lt;dbl&gt;,\n#   tectonic_settings_Rift.zone...Oceanic.crust....15.km. &lt;dbl&gt;,\n#   tectonic_settings_Subduction.zone...Continental.crust...25.km. &lt;dbl&gt;,\n#   tectonic_settings_Subduction.zone...Oceanic.crust....15.km. &lt;dbl&gt;,\n#   tectonic_settings_other &lt;dbl&gt;, major_rock_1_Basalt...Picro.Basalt &lt;dbl&gt;, …\n\n\nBefore utilizing prep(), the defined steps have been outlined but not executed. The evaluation of these steps takes place within the prep() function. The juice() function can then be employed to retrieve the preprocessed data and inspect the results.\nNow, the model specification is the focus. In this example, a workflow() is used for convenience. Workflows are objects designed to facilitate the management of modeling pipelines, with components that seamlessly fit together like Lego blocks. This particular workflow() includes both the recipe and the model, specifically a random forest classifier. The ranger implementation for random forests can handle multinomial classification without requiring any special handling.\n\n\nCode\nrf_spec &lt;- rand_forest(trees = 1000) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\")\n\nvolcano_wf &lt;- workflow() %&gt;%\n  add_recipe(volcano_rec) %&gt;%\n  add_model(rf_spec)\n\nvolcano_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_other()\n• step_other()\n• step_dummy()\n• step_zv()\n• step_normalize()\n• step_smote()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n\n\nFitting workflow to our resamples.\n\n\nCode\nvolcano_res &lt;- fit_resamples(\n  volcano_wf,\n  resamples = volcano_boot,\n  control = control_resamples(save_pred = TRUE)\n)\n\n\n\n\n\nA significant distinction when dealing with multiclass problems lies in the utilization of different performance metrics. The yardstick package offers implementations for a variety of multiclass metrics.\n\n\nCode\nvolcano_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass 0.642    25 0.00466 Preprocessor1_Model1\n2 roc_auc  hand_till  0.789    25 0.00321 Preprocessor1_Model1\n\n\nA confusion matrix can be generated to assess the performance of the various classes.\n\n\nCode\nvolcano_res %&gt;%\n  collect_predictions() %&gt;%\n  conf_mat(volcano_type, .pred_class)\n\n\n               Truth\nPrediction      Other Shield Stratovolcano\n  Other          1978    330           824\n  Shield          364    576           313\n  Stratovolcano  1144    172          3099\n\n\nEven with the application of SMOTE oversampling, identifying stratovolcanoes remains relatively straightforward.\nWhile accuracy and AUC were computed during fit_resamples(), it’s possible to revisit and calculate additional metrics of interest if the predictions were saved. Additionally, using group_by() on resamples allows for further analysis. Perform the same actions again.\n\n\nCode\nvolcano_res %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  ppv(volcano_type, .pred_class)\n\n\n# A tibble: 25 × 4\n   id          .metric .estimator .estimate\n   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n 1 Bootstrap01 ppv     macro          0.584\n 2 Bootstrap02 ppv     macro          0.597\n 3 Bootstrap03 ppv     macro          0.622\n 4 Bootstrap04 ppv     macro          0.552\n 5 Bootstrap05 ppv     macro          0.590\n 6 Bootstrap06 ppv     macro          0.624\n 7 Bootstrap07 ppv     macro          0.608\n 8 Bootstrap08 ppv     macro          0.584\n 9 Bootstrap09 ppv     macro          0.616\n10 Bootstrap10 ppv     macro          0.579\n# ℹ 15 more rows\n\n\nExploring some variable importance.\n\n\nCode\nlibrary(vip)\n\nrf_spec %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  fit(\n    volcano_type ~ .,\n    data = juice(volcano_prep) %&gt;%\n      select(-volcano_number) %&gt;%\n      janitor::clean_names()\n  ) %&gt;%\n  vip(geom = \"point\")\n\n\n\n\n\nThe spatial information holds significant importance for the model, with the presence of basalt being the next crucial factor. To delve deeper into the spatial information, let’s further explore it and create a map illustrating the accuracy or inaccuracy of our modeling across the world. We can achieve this by rejoining the predictions back to the original data. Repeat this process once again.\n\n\nCode\nvolcano_pred &lt;- volcano_res %&gt;%\n  collect_predictions() %&gt;%\n  mutate(correct = volcano_type == .pred_class) %&gt;%\n  left_join(volcano_df %&gt;%\n    mutate(.row = row_number()))\n\nvolcano_pred\n\n\n# A tibble: 8,800 × 15\n   id          .pred_Other .pred_Shield .pred_Stratovolcano  .row .pred_class  \n   &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;        \n 1 Bootstrap01       0.164       0.0844               0.752     8 Stratovolcano\n 2 Bootstrap01       0.228       0.0559               0.716    10 Stratovolcano\n 3 Bootstrap01       0.580       0.244                0.175    11 Other        \n 4 Bootstrap01       0.277       0.176                0.546    16 Stratovolcano\n 5 Bootstrap01       0.247       0.421                0.333    17 Shield       \n 6 Bootstrap01       0.216       0.439                0.344    22 Shield       \n 7 Bootstrap01       0.210       0.608                0.182    26 Shield       \n 8 Bootstrap01       0.213       0.0934               0.694    27 Stratovolcano\n 9 Bootstrap01       0.291       0.0521               0.657    28 Stratovolcano\n10 Bootstrap01       0.332       0.560                0.108    29 Shield       \n# ℹ 8,790 more rows\n# ℹ 9 more variables: volcano_type &lt;fct&gt;, .config &lt;chr&gt;, correct &lt;lgl&gt;,\n#   volcano_number &lt;dbl&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, elevation &lt;dbl&gt;,\n#   tectonic_settings &lt;fct&gt;, major_rock_1 &lt;fct&gt;\n\n\nNext, let’s generate a map using stat_summary_hex(). Within each hexagon, we’ll calculate the mean of correctness to determine the percentage of volcanoes that were classified correctly across all our bootstrap resamples.\n\n\n\n\nCode\nggplot() +\n  geom_map(\n    data = world, map = world,\n    aes(long, lat, map_id = region),\n    color = \"white\", fill = \"grey90\", size = 0.05, alpha = 0.5\n  ) +\n  stat_summary_hex(\n    data = volcano_pred,\n    aes(longitude, latitude, z = as.integer(correct)),\n    fun = \"mean\",\n    alpha = 0.7, bins = 50\n  ) +\n  scale_fill_gradient(high = \"red\", labels = scales::percent) +\n  theme_void(base_family = \"IBMPlexSans\") +\n  labs(x = NULL, y = NULL, fill = \"Percent classified\\ncorrectly\")"
  },
  {
    "objectID": "posts/Volcano/Volcano project.html#building-a-model",
    "href": "posts/Volcano/Volcano project.html#building-a-model",
    "title": "Multinomial Volcano Classification project",
    "section": "",
    "text": "Instead of dividing this relatively small dataset into training and testing data, a set of bootstrap resamples will be generated.\n\n\nCode\nlibrary(tidymodels)\nvolcano_boot &lt;- bootstraps(volcano_df)\n\nvolcano_boot\n\n\n# Bootstrap sampling \n# A tibble: 25 × 2\n   splits            id         \n   &lt;list&gt;            &lt;chr&gt;      \n 1 &lt;split [958/352]&gt; Bootstrap01\n 2 &lt;split [958/358]&gt; Bootstrap02\n 3 &lt;split [958/362]&gt; Bootstrap03\n 4 &lt;split [958/360]&gt; Bootstrap04\n 5 &lt;split [958/347]&gt; Bootstrap05\n 6 &lt;split [958/358]&gt; Bootstrap06\n 7 &lt;split [958/347]&gt; Bootstrap07\n 8 &lt;split [958/362]&gt; Bootstrap08\n 9 &lt;split [958/344]&gt; Bootstrap09\n10 &lt;split [958/349]&gt; Bootstrap10\n# ℹ 15 more rows\n\n\nOur multinomial classification model will be trained on these resamples. Next, our data will be preprocessed using a recipe. Considering the substantial imbalance with significantly fewer shield volcanoes compared to the other groups, SMOTE upsampling (via the themis package) will be applied to balance the classes.\n\n\nCode\nlibrary(themis)\n\nvolcano_rec &lt;- recipe(volcano_type ~ ., data = volcano_df) %&gt;%\n  update_role(volcano_number, new_role = \"Id\") %&gt;%\n  step_other(tectonic_settings) %&gt;%\n  step_other(major_rock_1) %&gt;%\n  step_dummy(tectonic_settings, major_rock_1) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_smote(volcano_type)\n\n\n\nFirst, the recipe() function needs to be informed about the model using a formula and the data it will work with.\nFollowing that, an update is made to the role of the volcano number, designating it as a variable to retain for convenience as an identifier for rows, although it is neither a predictor nor an outcome.\nConsidering the numerous tectonic settings and rocks in the dataset, less frequently occurring levels are combined into an “Other” category for each predictor.\nSubsequently, indicator variables are created, and those with zero variance are removed.\nPrior to oversampling, all predictors are centered and scaled (normalized).\nLastly, SMOTE oversampling is implemented to ensure balance among the volcano types.\n\n\n\nCode\nvolcano_prep &lt;- prep(volcano_rec)\njuice(volcano_prep)\n\n\n# A tibble: 1,383 × 14\n   volcano_number latitude longitude elevation volcano_type \n            &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;        \n 1         283001   0.618      0.984   -0.875  Shield       \n 2         355096  -1.21      -0.830    2.97   Stratovolcano\n 3         342080  -0.0153    -1.04     1.50   Stratovolcano\n 4         213004   0.746      0.101   -0.131  Other        \n 5         321040   0.988     -1.32     1.34   Stratovolcano\n 6         283170   0.718      1.06    -0.0992 Stratovolcano\n 7         221170  -0.156      0.158   -0.0956 Stratovolcano\n 8         221110  -0.0601     0.158   -0.440  Stratovolcano\n 9         284160   0.120      1.11    -0.644  Stratovolcano\n10         342100  -0.0165    -1.04     1.35   Stratovolcano\n# ℹ 1,373 more rows\n# ℹ 9 more variables:\n#   tectonic_settings_Rift.zone...Continental.crust...25.km. &lt;dbl&gt;,\n#   tectonic_settings_Rift.zone...Oceanic.crust....15.km. &lt;dbl&gt;,\n#   tectonic_settings_Subduction.zone...Continental.crust...25.km. &lt;dbl&gt;,\n#   tectonic_settings_Subduction.zone...Oceanic.crust....15.km. &lt;dbl&gt;,\n#   tectonic_settings_other &lt;dbl&gt;, major_rock_1_Basalt...Picro.Basalt &lt;dbl&gt;, …\n\n\nBefore utilizing prep(), the defined steps have been outlined but not executed. The evaluation of these steps takes place within the prep() function. The juice() function can then be employed to retrieve the preprocessed data and inspect the results.\nNow, the model specification is the focus. In this example, a workflow() is used for convenience. Workflows are objects designed to facilitate the management of modeling pipelines, with components that seamlessly fit together like Lego blocks. This particular workflow() includes both the recipe and the model, specifically a random forest classifier. The ranger implementation for random forests can handle multinomial classification without requiring any special handling.\n\n\nCode\nrf_spec &lt;- rand_forest(trees = 1000) %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\")\n\nvolcano_wf &lt;- workflow() %&gt;%\n  add_recipe(volcano_rec) %&gt;%\n  add_model(rf_spec)\n\nvolcano_wf\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_other()\n• step_other()\n• step_dummy()\n• step_zv()\n• step_normalize()\n• step_smote()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  trees = 1000\n\nComputational engine: ranger \n\n\nFitting workflow to our resamples.\n\n\nCode\nvolcano_res &lt;- fit_resamples(\n  volcano_wf,\n  resamples = volcano_boot,\n  control = control_resamples(save_pred = TRUE)\n)"
  },
  {
    "objectID": "posts/Volcano/Volcano project.html#exploring-results",
    "href": "posts/Volcano/Volcano project.html#exploring-results",
    "title": "Multinomial Volcano Classification project",
    "section": "",
    "text": "A significant distinction when dealing with multiclass problems lies in the utilization of different performance metrics. The yardstick package offers implementations for a variety of multiclass metrics.\n\n\nCode\nvolcano_res %&gt;%\n  collect_metrics()\n\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy multiclass 0.642    25 0.00466 Preprocessor1_Model1\n2 roc_auc  hand_till  0.789    25 0.00321 Preprocessor1_Model1\n\n\nA confusion matrix can be generated to assess the performance of the various classes.\n\n\nCode\nvolcano_res %&gt;%\n  collect_predictions() %&gt;%\n  conf_mat(volcano_type, .pred_class)\n\n\n               Truth\nPrediction      Other Shield Stratovolcano\n  Other          1978    330           824\n  Shield          364    576           313\n  Stratovolcano  1144    172          3099\n\n\nEven with the application of SMOTE oversampling, identifying stratovolcanoes remains relatively straightforward.\nWhile accuracy and AUC were computed during fit_resamples(), it’s possible to revisit and calculate additional metrics of interest if the predictions were saved. Additionally, using group_by() on resamples allows for further analysis. Perform the same actions again.\n\n\nCode\nvolcano_res %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  ppv(volcano_type, .pred_class)\n\n\n# A tibble: 25 × 4\n   id          .metric .estimator .estimate\n   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n 1 Bootstrap01 ppv     macro          0.584\n 2 Bootstrap02 ppv     macro          0.597\n 3 Bootstrap03 ppv     macro          0.622\n 4 Bootstrap04 ppv     macro          0.552\n 5 Bootstrap05 ppv     macro          0.590\n 6 Bootstrap06 ppv     macro          0.624\n 7 Bootstrap07 ppv     macro          0.608\n 8 Bootstrap08 ppv     macro          0.584\n 9 Bootstrap09 ppv     macro          0.616\n10 Bootstrap10 ppv     macro          0.579\n# ℹ 15 more rows\n\n\nExploring some variable importance.\n\n\nCode\nlibrary(vip)\n\nrf_spec %&gt;%\n  set_engine(\"ranger\", importance = \"permutation\") %&gt;%\n  fit(\n    volcano_type ~ .,\n    data = juice(volcano_prep) %&gt;%\n      select(-volcano_number) %&gt;%\n      janitor::clean_names()\n  ) %&gt;%\n  vip(geom = \"point\")\n\n\n\n\n\nThe spatial information holds significant importance for the model, with the presence of basalt being the next crucial factor. To delve deeper into the spatial information, let’s further explore it and create a map illustrating the accuracy or inaccuracy of our modeling across the world. We can achieve this by rejoining the predictions back to the original data. Repeat this process once again.\n\n\nCode\nvolcano_pred &lt;- volcano_res %&gt;%\n  collect_predictions() %&gt;%\n  mutate(correct = volcano_type == .pred_class) %&gt;%\n  left_join(volcano_df %&gt;%\n    mutate(.row = row_number()))\n\nvolcano_pred\n\n\n# A tibble: 8,800 × 15\n   id          .pred_Other .pred_Shield .pred_Stratovolcano  .row .pred_class  \n   &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;               &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;        \n 1 Bootstrap01       0.164       0.0844               0.752     8 Stratovolcano\n 2 Bootstrap01       0.228       0.0559               0.716    10 Stratovolcano\n 3 Bootstrap01       0.580       0.244                0.175    11 Other        \n 4 Bootstrap01       0.277       0.176                0.546    16 Stratovolcano\n 5 Bootstrap01       0.247       0.421                0.333    17 Shield       \n 6 Bootstrap01       0.216       0.439                0.344    22 Shield       \n 7 Bootstrap01       0.210       0.608                0.182    26 Shield       \n 8 Bootstrap01       0.213       0.0934               0.694    27 Stratovolcano\n 9 Bootstrap01       0.291       0.0521               0.657    28 Stratovolcano\n10 Bootstrap01       0.332       0.560                0.108    29 Shield       \n# ℹ 8,790 more rows\n# ℹ 9 more variables: volcano_type &lt;fct&gt;, .config &lt;chr&gt;, correct &lt;lgl&gt;,\n#   volcano_number &lt;dbl&gt;, latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, elevation &lt;dbl&gt;,\n#   tectonic_settings &lt;fct&gt;, major_rock_1 &lt;fct&gt;\n\n\nNext, let’s generate a map using stat_summary_hex(). Within each hexagon, we’ll calculate the mean of correctness to determine the percentage of volcanoes that were classified correctly across all our bootstrap resamples.\n\n\n\n\nCode\nggplot() +\n  geom_map(\n    data = world, map = world,\n    aes(long, lat, map_id = region),\n    color = \"white\", fill = \"grey90\", size = 0.05, alpha = 0.5\n  ) +\n  stat_summary_hex(\n    data = volcano_pred,\n    aes(longitude, latitude, z = as.integer(correct)),\n    fun = \"mean\",\n    alpha = 0.7, bins = 50\n  ) +\n  scale_fill_gradient(high = \"red\", labels = scales::percent) +\n  theme_void(base_family = \"IBMPlexSans\") +\n  labs(x = NULL, y = NULL, fill = \"Percent classified\\ncorrectly\")"
  }
]